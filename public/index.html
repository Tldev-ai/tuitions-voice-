<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>iiTuitions ‚Äî Voice</title>
<style>
  :root{ --ink:#000; --ring:#e5e7eb; --idle:#bbb }
  *{ box-sizing:border-box }
  body{ margin:0; font:16px/1.45 system-ui,-apple-system,Segoe UI,Roboto,Inter,sans-serif;
        color:var(--ink); background:linear-gradient(180deg,#9EC3F2 0%,#DCE7F7 100%); min-height:100vh }
  .wrap{ max-width:980px; margin:28px auto; padding:0 16px }
  .card{ background:#fff; border:1px solid var(--ring); border-radius:16px; padding:24px;
         box-shadow:0 6px 28px rgba(0,0,0,.06) }
  h1{ margin:0 0 10px }
  label{ display:flex; align-items:center; gap:8px; margin:10px 0 16px }
  .row{ display:flex; gap:12px; align-items:center; flex-wrap:wrap }
  button{ display:inline-flex; align-items:center; justify-content:center; gap:8px;
          padding:14px 32px; border:0; border-radius:999px; background:#000; color:#fff; font-weight:800; cursor:pointer }
  button:disabled{ opacity:.6; cursor:not-allowed }
  .dot{ width:10px; height:10px; border-radius:999px; background:var(--idle) }
  #log{ width:100%; height:280px; background:#fafafa; border:1px solid #eee; border-radius:12px; padding:12px;
        font:13px/1.35 ui-monospace,SFMono-Regular,Menlo,monospace; overflow:auto }
  audio{ display:none }
</style>
</head>
<body>
  <main class="wrap">
    <section class="card">
      <h1>Talk to our Admissions Assistant</h1>

      <label><input type="checkbox" id="consent" checked /> I agree to the recording and storage of this conversation.</label>

      <div class="row">
        <button id="startBtn">üéôÔ∏è Start</button>
        <button id="endBtn" disabled>End</button>
        <span id="led" class="dot" title="Mic activity"></span>
        <audio id="aiAudio" autoplay playsinline></audio>
      </div>

      <p style="margin:10px 0 6px">On call. Speak normally. The assistant will reply.</p>
      <pre id="log"></pre>
    </section>
  </main>

<script>
const startBtn = document.getElementById('startBtn');
const endBtn   = document.getElementById('endBtn');
const aiAudio  = document.getElementById('aiAudio');
const consent  = document.getElementById('consent');
const led      = document.getElementById('led');
const logEl    = document.getElementById('log');

let pc, dc, localStream;

function log(...a){
  const line = a.map(x => typeof x === 'object' ? JSON.stringify(x) : String(x)).join(' ');
  logEl.textContent += line + '\n';
  logEl.scrollTop = logEl.scrollHeight;
  console.log(...a);
}

function timeOfDay(){ const h=new Date().getHours(); return h<12?'morning':h<17?'afternoon':h<21?'evening':'night'; }

async function startCall(){
  if (!consent.checked){ alert('Please agree to recording to proceed.'); return; }
  startBtn.disabled = true; endBtn.disabled = false;

  // 1) Fetch ephemeral session (already configured to speak)
  const s = await fetch('/api/session');
  if (!s.ok){
    const t = await s.text();
    alert('Failed to create session:\n' + t);
    startBtn.disabled=false; endBtn.disabled=true; return;
  }
  const sess = await s.json();
  const TOKEN = sess.client_secret?.value;
  const MODEL = encodeURIComponent(sess.model || 'gpt-4o-realtime-preview');

  // 2) Peer connection
  pc = new RTCPeerConnection({
    iceServers: [
      { urls: ['stun:stun.l.google.com:19302'] },
      // If you later add TURN, include them here
      // { urls: 'turn:YOUR_TURN_HOST:3478', username: 'u', credential: 'p' }
    ]
  });

  pc.oniceconnectionstatechange = () => log('ice:', pc.iceConnectionState);
  pc.onconnectionstatechange   = () => log('pc:',  pc.connectionState);

  // MUST announce that we want to both send and receive audio
  pc.addTransceiver('audio', { direction: 'sendrecv' });

  // 3) Data channel for events
  dc = pc.createDataChannel('oai-events');
  dc.onmessage = (e) => {
    try {
      const msg = JSON.parse(e.data);
      if (msg?.type?.includes?.('error') || msg?.level === 'error') log('OAI error:', msg);
      else if (msg?.type) log('OAI:', msg.type);
      else log('OAI:', e.data);
    } catch { log('OAI:', e.data); }
  };
  dc.onopen = () => {
    // Re-assert audio every time just in case
    dc.send(JSON.stringify({
      type: 'session.update',
      session: {
        modalities: ['audio','text'],
        voice: 'verse',
        turn_detection: { type: 'server_vad', silence_duration_ms: 800 }
      }
    }));
    // Explicit audio greeting (guarantees first turn is spoken)
    dc.send(JSON.stringify({
      type: 'response.create',
      response: {
        modalities: ['audio','text'],
        instructions: `Hai. Good ${timeOfDay()}. Which language would you like to talk in ‚Äî English, ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å (Telugu), or ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (Hindi)?`
      }
    }));
  };

  // 4) Capture mic and publish
  try {
    localStream = await navigator.mediaDevices.getUserMedia({
      audio: { echoCancellation:true, noiseSuppression:true, autoGainControl:true }
    });
  } catch {
    alert('Microphone blocked. Allow mic access and try again.');
    startBtn.disabled=false; endBtn.disabled=true; return;
  }
  localStream.getTracks().forEach(t => pc.addTrack(t, localStream));

  // LED blink on voice activity (client-side rough indicator)
  const ctx = new (window.AudioContext || window.webkitAudioContext)();
  const analyser = ctx.createAnalyser(); analyser.fftSize=2048;
  const micTap = ctx.createMediaStreamSource(localStream); micTap.connect(analyser);
  const arr = new Float32Array(analyser.fftSize);
  (function tick(){
    analyser.getFloatTimeDomainData(arr);
    let s=0; for(let i=0;i<arr.length;i++){ s+=arr[i]*arr[i]; }
    const rms = Math.sqrt(s/arr.length);
    led.style.background = rms>0.01 ? '#000' : '#bbb';
    requestAnimationFrame(tick);
  })();

  // 5) Remote audio
  pc.ontrack = (ev) => {
    const stream = ev.streams?.[0];
    if (stream) {
      aiAudio.srcObject = stream;
      aiAudio.muted = false;
      aiAudio.play().catch(()=>{});
      log('remote track unmuted (audio flowing)');
    }
  };

  // 6) SDP offer ‚Üí OpenAI ‚Üí answer
  const offer = await pc.createOffer();
  await pc.setLocalDescription(offer);
  await new Promise((resolve) => {
    if (pc.iceGatheringState === 'complete') return resolve();
    pc.onicegatheringstatechange = () => {
      if (pc.iceGatheringState === 'complete') resolve();
    };
    setTimeout(resolve, 1500);
  });

  const r = await fetch(`https://api.openai.com/v1/realtime?model=${MODEL}`, {
    method: 'POST',
    headers: {
      Authorization: `Bearer ${TOKEN}`,
      'Content-Type': 'application/sdp',
      'OpenAI-Beta': 'realtime=v1'
    },
    body: pc.localDescription.sdp
  });

  if (!r.ok){
    const t = await r.text();
    alert('Realtime connection failed:\n' + t);
    endCall();
    return;
  }
  const answer = { type: 'answer', sdp: await r.text() };
  await pc.setRemoteDescription(answer);
}

function endCall(){
  try { dc && dc.close(); } catch{}
  try { pc && pc.close(); } catch{}
  try { localStream && localStream.getTracks().forEach(t => t.stop()); } catch{}
  startBtn.disabled = false; endBtn.disabled = true;
}

startBtn.addEventListener('click', startCall);
endBtn.addEventListener('click', endCall);
window.addEventListener('beforeunload', endCall);
</script>
</body>
</html>
