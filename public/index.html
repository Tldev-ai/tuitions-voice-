<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <meta http-equiv="Cache-Control" content="no-store" />
  <title>iiTuitions — Voice</title>
  <style>
    :root{ --ink:#000; --ring:#e5e7eb; --idle:#bbb; }
    *{box-sizing:border-box}
    body{ margin:0; color:var(--ink); font:16px/1.45 system-ui,-apple-system,Segoe UI,Roboto,Inter,sans-serif;
      background:linear-gradient(180deg,#9EC3F2 0%,#DCE7F7 100%); min-height:100vh; }
    .top{ display:flex;align-items:center;gap:10px; border-bottom:1px solid var(--ring);
      padding:12px 16px; background:#ffffffcc; backdrop-filter:saturate(120%) blur(6px); }
    .brand{display:flex;align-items:center;gap:8px;text-decoration:none;color:var(--ink);font-weight:800}
    .wrap{max-width:980px;margin:28px auto;padding:0 16px}
    .card{padding:24px;border:1px solid var(--ring);border-radius:16px;box-shadow:0 6px 28px rgba(0,0,0,.06);background:#fff}
    h1{margin:0 0 10px}
    .row{display:flex;gap:12px;align-items:center;margin-top:12px;flex-wrap:wrap}
    button{display:inline-flex;gap:8px;align-items:center;justify-content:center;padding:12px 16px;border-radius:999px;border:0;
      background:#000;color:#fff;font-weight:800;cursor:pointer;letter-spacing:.2px}
    #startBtn{min-width:320px;padding:14px 36px;font-size:18px}
    button.end{background:#000;color:#fff;border:2px solid #000}
    button:disabled{opacity:.6;cursor:not-allowed}
    audio{display:none}
    .led{width:10px;height:10px;border-radius:999px;background:var(--idle);box-shadow:0 0 0 1px #0001}
    .led.on{background:#000}
  </style>
</head>
<body>
  <header class="top">
    <a class="brand" href="#"><img src="/assets/iituitions-logo.png" alt="logo" style="height:24px"> iiTuitions</a>
  </header>

  <main class="wrap">
    <section class="card">
      <h1>Talk to our Admissions Assistant</h1>
      <label style="display:flex;align-items:center;gap:8px;margin-top:10px">
        <input type="checkbox" id="consent" />
        I agree to the recording and storage of this conversation.
      </label>
      <div class="row">
        <button id="startBtn">Start Voice</button>
        <button id="endBtn" class="end" disabled>End</button>
        <span class="led" id="led" title="Mic activity"></span>
        <audio id="aiAudio" autoplay playsinline></audio>
      </div>
    </section>
  </main>

  <script>
    const startBtn = document.getElementById('startBtn');
    const endBtn   = document.getElementById('endBtn');
    const aiAudio  = document.getElementById('aiAudio');
    const consent  = document.getElementById('consent');
    const led      = document.getElementById('led');

    let pc, dc, localStream, remoteStream;
    let analyser, dataArray, silenceRAF, silenceTimer;
    let callSeq = 0;

    const SILENCE_MS = 15000, RMS_THRESHOLD = 0.006;
    const tod = ()=>{ const h=new Date().getHours(); return h<12?'morning':h<17?'afternoon':h<21?'evening':'night'; };
    const log = (...a)=>console.log(...a);

    async function startCall() {
      if (!consent.checked) { alert('Please provide consent to proceed.'); return; }
      startBtn.disabled = true; endBtn.disabled = false;
      const mySeq = ++callSeq;

      // 1) Create ephemeral session (and receive ICE servers)
      const sessResp = await fetch('/api/session', { cache: 'no-store' });
      if (!sessResp.ok) {
        const err = await sessResp.json().catch(()=>({}));
        alert('Failed to create session' + (err?.error?.message ? (':\n' + err.error.message) : ''));
        startBtn.disabled=false; endBtn.disabled=true; return;
      }
      const sess = await sessResp.json();
      const EPHEMERAL = sess?.client_secret?.value;
      const MODEL = encodeURIComponent(sess?.model || 'gpt-4o-realtime-preview');
      const iceServers = sess?.ice_servers || [{ urls: ['stun:stun.l.google.com:19302'] }];

      // 2) Audio context (resume on user gesture)
      const ctx = new (window.AudioContext || window.webkitAudioContext)();
      if (ctx.state === 'suspended') { try { await ctx.resume(); } catch {} }

      // 3) PeerConnection
      pc = new RTCPeerConnection({ iceServers /*, iceTransportPolicy:'relay'*/ });
      window.pc = pc;
      pc.oniceconnectionstatechange = () => log('ice:', pc.iceConnectionState);
      pc.onconnectionstatechange    = () => log('pc:', pc.connectionState);

      // 4) DataChannel for events
      dc = pc.createDataChannel('oai-events');
      dc.onmessage = (e) => {
        try {
          const msg = JSON.parse(e.data);
          console.log('OAI:', msg.type || msg);

          // After server-VAD says speech stopped, ask model to respond
          if (msg.type === 'input_audio_buffer.speech_stopped') {
            maybeAskForResponse();
          }

          // If we see the user utterance committed into the conversation, also trigger
          if (msg.type === 'conversation.item.created' && msg.item?.role === 'user') {
            maybeAskForResponse();
          }
        } catch { console.log('OAI raw:', e.data); }
      };

      let requested = false;
      const maybeAskForResponse = () => {
        if (!dc || dc.readyState !== 'open') return;
        // Rate-limit double-fires from both events above
        if (requested) return; requested = true;
        dc.send(JSON.stringify({
          type: 'response.create',
          response: { modalities: ['audio','text'] }
        }));
        setTimeout(()=>{ requested = false; }, 400); // small window to avoid duplicates
      };

      dc.onopen = () => {
        // Optional session.update (voice/modality)
        dc.send(JSON.stringify({
          type: 'session.update',
          session: { modalities: ['audio','text'], voice: 'verse', turn_detection: { type:'server_vad', silence_duration_ms: 700 } }
        }));
        // First greeting
        dc.send(JSON.stringify({
          type: 'response.create',
          response: { modalities:['audio','text'], instructions: `Hai. Good ${tod()}. May I record this short call for admission support? Which language would you like — English, తెలుగు (Telugu), or हिन्दी (Hindi)?` }
        }));
      };

      // 5) Mic → send
      try {
        localStream = await navigator.mediaDevices.getUserMedia({
          audio: { echoCancellation:true, noiseSuppression:true, autoGainControl:true }
        });
      } catch {
        alert('Microphone blocked. Click Allow and reload.');
        startBtn.disabled=false; endBtn.disabled=true; return;
      }
      localStream.getTracks().forEach(t => pc.addTrack(t, localStream));

      // 6) Remote audio → play (via <audio> AND WebAudio to bypass autoplay blocks)
      pc.ontrack = (ev) => {
        const stream = ev.streams?.[0] || new MediaStream([ev.track]);
        if (!remoteStream) remoteStream = stream;

        aiAudio.srcObject = remoteStream;
        aiAudio.muted = false;
        aiAudio.volume = 1.0;
        aiAudio.play().catch(err => console.warn('audio.play failed:', err));

        try {
          const remoteNode = ctx.createMediaStreamSource(remoteStream);
          remoteNode.connect(ctx.destination); // <- guaranteed audible path
        } catch (e) { console.warn('WebAudio connect failed:', e); }

        ev.track.onunmute = () => log('remote track unmuted (audio flowing)');

        // LED + silence from mic (optional)
        const analyser = ctx.createAnalyser(); analyser.fftSize = 2048;
        const micTap = ctx.createMediaStreamSource(localStream);
        const dataArray = new Float32Array(analyser.fftSize);
        micTap.connect(analyser);

        const resetSilence = () => {
          if (silenceTimer) clearTimeout(silenceTimer);
          silenceTimer = setTimeout(() => {
            try {
              dc?.send(JSON.stringify({
                type:'response.create',
                response:{ modalities:['audio','text'], instructions:"Sorry, I'm unable to hear you. I'll end this call now." }
              }));
            } catch {}
          }, SILENCE_MS);
        };

        const monitor = () => {
          analyser.getFloatTimeDomainData(dataArray);
          let rms = 0; for (let i=0;i<dataArray.length;i++){ const v=dataArray[i]; rms += v*v; }
          rms = Math.sqrt(rms / dataArray.length);
          if (rms > RMS_THRESHOLD) { led.classList.add('on'); resetSilence(); }
          else { led.classList.remove('on'); }
          silenceRAF = requestAnimationFrame(monitor);
        };
        setTimeout(()=>{ resetSilence(); monitor(); }, 800);
      };

      // 7) SDP offer/answer with OpenAI
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);
      await new Promise((resolve) => {
        if (pc.iceGatheringState === 'complete') return resolve();
        pc.onicegatheringstatechange = () => { if (pc.iceGatheringState === 'complete') resolve(); };
        setTimeout(resolve, 1500);
      });

      const sdpResp = await fetch(`https://api.openai.com/v1/realtime?model=${MODEL}`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${EPHEMERAL}`,
          'Content-Type': 'application/sdp',
          'OpenAI-Beta': 'realtime=v1'
        },
        body: pc.localDescription.sdp
      });

      if (!sdpResp.ok) {
        console.error('Realtime SDP error:', await sdpResp.text());
        alert('Realtime connection failed (model/quota/key).');
        endCall(); return;
      }

      const sdpText = await sdpResp.text();
      if (mySeq !== callSeq) return;
      if (pc.signalingState === 'have-local-offer') {
        await pc.setRemoteDescription({ type: 'answer', sdp: sdpText });
      } else {
        console.warn('Skipping setRemoteDescription; signalingState=', pc.signalingState);
      }
    }

    async function endCall() {
      endBtn.disabled = true;
      callSeq++;
      try { if (silenceTimer) clearTimeout(silenceTimer); if (silenceRAF) cancelAnimationFrame(silenceRAF); } catch {}
      try { pc && pc.close(); } catch {}
      try { localStream && localStream.getTracks().forEach(t => t.stop()); } catch {}
      startBtn.disabled = false;
      led.classList.remove('on');

      // optional: send your recording blob here – we stub the endpoint below
      // await fetch('/api/upload', { method:'POST', body: formData })
    }

    startBtn.addEventListener('click', startCall);
    endBtn.addEventListener('click', endCall);
    window.addEventListener('beforeunload', () => { try { pc && pc.close(); } catch {} });
  </script>
</body>
</html>
